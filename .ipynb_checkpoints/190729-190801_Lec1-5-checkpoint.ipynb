{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ Lec 01 기본적 머신러닝 용어와 개념 설명\n",
    "\n",
    " ㅇ머신러닝: 데이터 기반으로 학습하여 컴퓨터가 프로그래밍\n",
    " ㅇ머신러닝의 종류: 학습 방식에 따라 레이블링된 데이터 기반의 지도학습(supervised learning),  un-labeled data 기반 비지도 학습\n",
    " ㅇ지도학습의 종류: classification(분류), regression(추정)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§Lec 02 Simple Linear Regression\n",
    "\n",
    " ㅇ 리그레션 개념: 후퇴, 되돌아 간다 -> ‘regression toward the mean’\n",
    " ㅇLinear Regression 선형회귀: 데이터를 가장 잘 대변하는 직선의 방정식을 찾는 것\n",
    "   특히 기울기와 y절편 찾는 것 \n",
    "   가설 H(x) = W*x + b\n",
    " ㅇ 비용 cost : H(x) - y    : 가설과 실제y값 차이\n",
    "     -> 오차가 음수 일수도 있으므로 제곱의 평균을 구하고, \n",
    "     -> Cost function을 최소화 -> 데이터를 가장 잘 대변하는 직선의 방정식 찾기\n",
    "     \n",
    "    𝑐𝑜𝑠𝑡(𝑊)=1/𝑚*∑𝑖=1𝑚(𝑊𝑥𝑖−𝑦𝑖)2\n",
    "    \n",
    "    *cost function = loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ Lec 03 Simple Linear Regression and how to minimize cost\n",
    " ㅇ Hypothesis 가설 모델 : H(x) = W*x + b   -> 학습: cost function 최소화\n",
    "\n",
    " ㅇ W(가중치 또는 기울기)에 따라 Cost function 변화 \n",
    "     -> Cost function 최소화 하는 W 찾기 -> 학습\n",
    " \n",
    " ㅇ Gradient descent algorithm\n",
    "    Cost function 최저점 찾기 위한 알고리즘\n",
    "    W 랜덤(초기값) 부여 -> 업데이트\n",
    "    이를 위해 그래디언트 디센트 알고리즘에서는 코스트펑션 W편미분값을 러닝레이트와 곱한 값을 기존값에서 빼면서 W 업데이트\n",
    "    *그래디언트 디센트 사용시 컨벡스 펑션(로컬미니멈 = 글로벌 미니멈) 아닌 경우 코스트펑션의 미니멈값 찾는데에 한계 발생함\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ Lec 04 Multivariable linear Regression\n",
    " ㅇ multivariate(feature) regression문제: 변수가 다수, 추정값은 하나 \n",
    "   -> 매트릭스 및 닷프로덕트 사용하여 다수의 피쳐 및 가중치 연산에 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "§ Lec 05 Logistic Regression\n",
    " ㅇ 개념\n",
    "    분류 -> discrete한 데이터 분류는 로지스틱리그레션\n",
    "    예측 -> continuous한 데이터 예측은 리니어 리그레션 사용\n",
    " ㅇ Hypothesis representation\n",
    "    파라메터 -> linear function -> logistic function -> decision boundary -> 분류값\n",
    "    과정을 통해 분류값을 구하게되며, 로지스틱 펑션으로 hypothesis representation\n",
    "    즉, 기존 펑션을 이산값 표현화하고, 디시젼 바운더리 기준으로 binary한 분류 가능하게 해줌\n",
    "\n",
    " ㅇ sigmoid\n",
    "    g(z) = 1/(1+e^(-z))\n",
    "    -> 로지스틱펑션 입력값 z가 커지면 1에 수렴, Z 작아지면 0에 수렴하여 1,0 으로 분류값 산출\n",
    "  \n",
    " ㅇ Cost function\n",
    "    - 컨벡스 함수화 필요\n",
    "    logistic regression에서도 cost function(추정값과 실제값의 차이) 최소화\n",
    "    추정값의 경우 Sigmoid 등 logistics function 아웃풋을 사용하므로, \n",
    "    0과 1사이의 연속적인 값\n",
    "    실제값의 경우 분류 레이블이므로 0 또는 1의 이산 값 \n",
    "    -> logistics function 아웃풋과 분류값 차이만을 사용하면 코스트펑션 컨벡스 하지 않음\n",
    "    \n",
    "    - 이러한 문제 해결위한 방법으로 log 및 실제 분류값 y와의 곱 활용\n",
    "      1. 분류값 1 -> 로지스틱 함수값 1이면 코스트펑션 0, 0에 가까울수록 무한대 발산\n",
    "      2. 분류값 0 -> 로지스틱 함수값 0이면 코스트펑션 0, 1에 가까울수록 무한대 발산\n",
    "     하는 코스트펑션 구성\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
